{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Implmentation of CBOW & GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import nltk\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "import os\n",
    "import os.path\n",
    "import io\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import spearmanr\n",
    "from nltk.corpus import stopwords\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from scipy import stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#CBOW Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloads\n",
    "\n",
    "#Downloaded the simplewiki-latest-pages-articles.xml.bg\n",
    "# Dated: 20-Feb-2023 16:13\n",
    "#Source: https://dumps.wikimedia.org/simplewiki/latest/simplewiki-latest-pages-articles.xml.bz2 \n",
    "\n",
    "#Used Wiki Extactor from and concatenated all text.\n",
    "#https://github.com/attardi/wikiextractor\n",
    "\n",
    "#Download the stopwords of English from nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['extracted_data', 'misc', 'stanford-corenlp-4.5.1', 'simplewiki-latest-pages-articles.txt', '.DS_Store', 'GloVe_model.model.vectors.npy', 'wikiextractor-master', 'cbow_model.model', 'simplewiki-latest-pages-articles.xml', 'stanford-corenlp-latest.zip', 'preprocessed_text.txt', 'wordsim_goldstandard.txt', 'wikiextractor-master.zip', 'glove-wiki-gigaword-100.txt', '.ipynb_checkpoints', 'GloVe_model.model', 'tokenized_text.txt', 'code_wiki.ipynb']\n"
     ]
    }
   ],
   "source": [
    "#Contents of the local folder\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the text\n",
    "with open(\"simplewiki-latest-pages-articles.txt\", \"r\",encoding=\"utf-16\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"id\": \"1\", \"revid\": \"440121\", \"url\": \"https://simple.wikipedia.org/wiki?curid=1\", \"title\": \"April\", \"text\": \"April is the fourth month of the year in the Julian and Gregorian calendars, and comes between March and May. It is one of four months to have 30 days.\\\\nApril always begins on the same day of week as July, and additionally, January in leap years. April always ends on the same day of the week as December.\\\\nApril\\'s flowers are the Sweet Pea and Daisy. Its birthstone is the diamond. The mea'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Peek the loaded text file.\n",
    "text[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the set of English stop words\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the input text into lines and parse each line as a JSON object\n",
    "json_list = [json.loads(line) for line in text.split('\\n') if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the text field from each dictionary and preprocess it\n",
    "preprocessed_text = []\n",
    "for j in json_list:\n",
    "    tokens = nltk.word_tokenize(j['text'])\n",
    "    filtered_tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
    "    preprocessed_text.append(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed_text to a file\n",
    "with open('preprocessed_text.txt', 'w') as f:\n",
    "    for line in preprocessed_text:\n",
    "        f.write(' '.join(line) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the type of processed text\n",
    "type(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Standford Tokenizer\n",
    "nlp = StanfordCoreNLP('http://localhost',port=9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text using the Stanford tokenizer\n",
    "tokenized_text = [nlp.word_tokenize(\" \".join(sentence)) for sentence in preprocessed_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed_text to a file\n",
    "with open('tokenized_text.txt', 'w') as f:\n",
    "    for line in tokenized_text:\n",
    "        f.write(' '.join(line) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a CBOW model on the tokenized sentences\n",
    "#Default parameters are used.\n",
    "cbow_model = Word2Vec(tokenized_text,vector_size=100,window=5,min_count=5,workers=4,sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens available : 314722\n",
      "Dimension of the word vector: 100\n"
     ]
    }
   ],
   "source": [
    "# Number of tokens available\n",
    "print(\"Number of Tokens available : \"+str(cbow_model.corpus_count))\n",
    "\n",
    "# Dimensions of the word vector \n",
    "print(\"Dimension of the word vector: \"+str(cbow_model.vector_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('elizabeth', 0.6839427947998047),\n",
       " ('crown', 0.6766826510429382),\n",
       " ('princess', 0.6750682592391968),\n",
       " ('king', 0.672218382358551),\n",
       " ('monarch', 0.652546226978302)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check top 5 most related words to queen\n",
    "cbow_model.wv.most_similar('queen', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6722183"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check relation between queen and king\n",
    "cbow_model.wv.similarity('queen','king')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save cbow model\n",
    "cbow_model.save('cbow_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate CBOW model and used final goldstandard of relatedness from\n",
	"#wordsim353(Finkelstein, et al., January 2002) collection\n",
	"#located at: http://alfonseca.org/eng/research/wordsim353.html \n",
    "Wordsim353_eval_file=\"wordsim_goldstandard.txt\"\n",
    "CBOW_model_scores_wordsim353 = []\n",
    "wordA=[]\n",
    "wordB=[]\n",
    "Wordsim353_Evalutaion_Filescore=[]\n",
    "with open(Wordsim353_eval_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.lower()\n",
    "        word1, word2, score = line.strip().split(\"\\t\")\n",
    "        if word1 in cbow_model.wv.index_to_key and word2 in cbow_model.wv.index_to_key:\n",
    "            relatedness_scores = cbow_model.wv.similarity(word1, word2)\n",
    "            wordA.append(word1)\n",
    "            wordB.append(word2)\n",
    "            Wordsim353_Evalutaion_Filescore.append(float(score))\n",
    "            CBOW_model_scores_wordsim353.append(relatedness_scores)\n",
    "        else:\n",
    "            print(\"Word not found, remove word from the Evaluation or increase Corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.709419689091693"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pearson correlation score\n",
    "stats.pearsonr(CBOW_model_scores_wordsim353,Wordsim353_Evalutaion_Filescore)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6970069308160629"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.spearmanr(CBOW_model_scores_wordsim353,Wordsim353_Evalutaion_Filescore)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#GloVe Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word-vectors from local file if avialable else download it from gensim-data\n",
    "try:\n",
    "    GloVe_model = KeyedVectors.load_word2vec_format('glove-wiki-gigaword-100.txt', binary=False)\n",
    "except:\n",
    "    GloVe_model = api.load(\"glove-wiki-gigaword-100\")\n",
    "    GloVe_model.save_word2vec_format('glove-wiki-gigaword-100.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens: 400000\n",
      "Dimension of a word vector: 100\n"
     ]
    }
   ],
   "source": [
    "# Printing out number of tokens available\n",
    "print(\"Number of Tokens: \"+str(GloVe_model.vectors.shape[0]))\n",
    "\n",
    "# Printing out the dimension of a word vector \n",
    "print(\"Dimension of a word vector: \"+str(GloVe_model.vectors.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('princess', 0.8973613977432251),\n",
       " ('king', 0.8753836750984192),\n",
       " ('elizabeth', 0.8677847385406494),\n",
       " ('royal', 0.8532505035400391),\n",
       " ('lady', 0.8522390127182007)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check top 5 most related words to queen\n",
    "GloVe_model.most_similar_cosmul('queen', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7507691"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Performing relationship test for king queen\n",
    "GloVe_model.similarity('queen', 'king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save GloVe model\n",
    "GloVe_model.save('GloVe_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate GloVe model and used final goldstandard of relatedness from\n",
	"#wordsim353(Finkelstein, et al., January 2002) collection\n",
	"#located at: http://alfonseca.org/eng/research/wordsim353.html \n",
    "Wordsim353_eval_file=\"wordsim_goldstandard.txt\"\n",
    "GloVe_model_scores_wordsim353 = []\n",
    "wordA=[]\n",
    "wordB=[]\n",
    "Wordsim353_Evalutaion_Filescore=[]\n",
    "with open(Wordsim353_eval_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.lower()\n",
    "        word1, word2, score = line.strip().split(\"\\t\")\n",
    "        if word1 in GloVe_model and word2 in GloVe_model:\n",
    "            relatedness_scores = GloVe_model.similarity(word1, word2)\n",
    "            wordA.append(word1)\n",
    "            wordB.append(word2)\n",
    "            Wordsim353_Evalutaion_Filescore.append(float(score))\n",
    "            GloVe_model_scores_wordsim353.append(relatedness_scores)\n",
    "        else:\n",
    "            print(\"Word not found, remove word from the Evaluation or increase Corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5941813212764435"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pearson correlation score\n",
    "stats.pearsonr(GloVe_model_scores_wordsim353,Wordsim353_Evalutaion_Filescore)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6035208452876207"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.spearmanr(GloVe_model_scores_wordsim353,Wordsim353_Evalutaion_Filescore)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32bdd722d48efc6e1953d89a7165a195d0186bf9f639485846378326bbac9548"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
